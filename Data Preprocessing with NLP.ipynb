{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fad94373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# //Data Preprocessing with nltk tool steps\n",
    "# 1st. Lower Case\n",
    "# 2nd Tokenization\n",
    "# 3rd  Stopwords Removal\n",
    "# 4th Stemmed & Lemmatization\n",
    "# 5th Removing Punctuation & Special Character\n",
    "# 6th Joining Tokens\n",
    "# 7th Text Vectorization (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b3cc0e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SIDDESH\n",
      "[nltk_data]     VICHARE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ca83173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (3.0.10)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ddab26",
   "metadata": {},
   "source": [
    "# Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "84608aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path=\"processed4_data.xlsx\"\n",
    "data = pd.read_excel(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ba996a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data['text'].str.lower()\n",
    "data[\"intent\"]=data['intent'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55231846",
   "metadata": {},
   "source": [
    "# Data Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a9cb747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text        intent  \\\n",
      "0                                  how are you??      greeting   \n",
      "1                                          hello      greeting   \n",
      "2                                     what's up?      greeting   \n",
      "3                                         thanks  appreciation   \n",
      "4                              thank you so much  appreciation   \n",
      "..                                           ...           ...   \n",
      "157  tell me a joke to lighten the mood.                  joke   \n",
      "158  do you have a funny joke?                            joke   \n",
      "159   share a joke with me.                               joke   \n",
      "160   give me a lighthearted joke.                        joke   \n",
      "161                               tell me a joke          joke   \n",
      "\n",
      "                                     tokenized_text  \\\n",
      "0                             [how, are, you, ?, ?]   \n",
      "1                                           [hello]   \n",
      "2                                 [what, 's, up, ?]   \n",
      "3                                          [thanks]   \n",
      "4                            [thank, you, so, much]   \n",
      "..                                              ...   \n",
      "157  [tell, me, a, joke, to, lighten, the, mood, .]   \n",
      "158              [do, you, have, a, funny, joke, ?]   \n",
      "159                   [share, a, joke, with, me, .]   \n",
      "160            [give, me, a, lighthearted, joke, .]   \n",
      "161                             [tell, me, a, joke]   \n",
      "\n",
      "                                filtered_text  \\\n",
      "0                                  ['?', '?']   \n",
      "1                                   ['hello']   \n",
      "2                                 [\"'s\", '?']   \n",
      "3                                  ['thanks']   \n",
      "4                           ['thank', 'much']   \n",
      "..                                        ...   \n",
      "157  ['tell', 'joke', 'lighten', 'mood', '.']   \n",
      "158                    ['funny', 'joke', '?']   \n",
      "159                    ['share', 'joke', '.']   \n",
      "160     ['give', 'lighthearted', 'joke', '.']   \n",
      "161                          ['tell', 'joke']   \n",
      "\n",
      "                                          stemmed_text  \\\n",
      "0                      ['how', 'are', 'you', '?', '?']   \n",
      "1                                            ['hello']   \n",
      "2                            ['what', \"'s\", 'up', '?']   \n",
      "3                                            ['thank']   \n",
      "4                       ['thank', 'you', 'so', 'much']   \n",
      "..                                                 ...   \n",
      "157  ['tell', 'me', 'a', 'joke', 'to', 'lighten', '...   \n",
      "158   ['do', 'you', 'have', 'a', 'funni', 'joke', '?']   \n",
      "159          ['share', 'a', 'joke', 'with', 'me', '.']   \n",
      "160     ['give', 'me', 'a', 'lightheart', 'joke', '.']   \n",
      "161                        ['tell', 'me', 'a', 'joke']   \n",
      "\n",
      "                                       lemmatized_text  \\\n",
      "0                              ['how', 'are', 'you??']   \n",
      "1                                            ['hello']   \n",
      "2                                    [\"what's\", 'up?']   \n",
      "3                                           ['thanks']   \n",
      "4                       ['thank', 'you', 'so', 'much']   \n",
      "..                                                 ...   \n",
      "157  ['tell', 'me', 'a', 'joke', 'to', 'lighten', '...   \n",
      "158       ['do', 'you', 'have', 'a', 'funny', 'joke?']   \n",
      "159              ['share', 'a', 'joke', 'with', 'me.']   \n",
      "160       ['give', 'me', 'a', 'lighthearted', 'joke.']   \n",
      "161                        ['tell', 'me', 'a', 'joke']   \n",
      "\n",
      "                                          cleaned_text  \n",
      "0                                ['how', 'are', 'you']  \n",
      "1                                            ['hello']  \n",
      "2                                      ['whats', 'up']  \n",
      "3                                           ['thanks']  \n",
      "4                       ['thank', 'you', 'so', 'much']  \n",
      "..                                                 ...  \n",
      "157  ['tell', 'me', 'a', 'joke', 'to', 'lighten', '...  \n",
      "158        ['do', 'you', 'have', 'a', 'funny', 'joke']  \n",
      "159               ['share', 'a', 'joke', 'with', 'me']  \n",
      "160        ['give', 'me', 'a', 'lighthearted', 'joke']  \n",
      "161                        ['tell', 'me', 'a', 'joke']  \n",
      "\n",
      "[162 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean the 'text' column by converting non-string values to strings\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "# Tokenize the 'text' column and create the 'tokenized_text' column\n",
    "data['tokenized_text'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Print the DataFrame to see the tokenized results\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ada4ff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are you??</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[how, are, you, ?, ?]</td>\n",
       "      <td>['?', '?']</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['how', 'are', 'you??']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[hello]</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[what, 's, up, ?]</td>\n",
       "      <td>[\"'s\", '?']</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"what's\", 'up?']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thank']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>[thank, you, so, much]</td>\n",
       "      <td>['thank', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text        intent          tokenized_text      filtered_text  \\\n",
       "0      how are you??      greeting   [how, are, you, ?, ?]         ['?', '?']   \n",
       "1              hello      greeting                 [hello]          ['hello']   \n",
       "2         what's up?      greeting       [what, 's, up, ?]        [\"'s\", '?']   \n",
       "3             thanks  appreciation                [thanks]         ['thanks']   \n",
       "4  thank you so much  appreciation  [thank, you, so, much]  ['thank', 'much']   \n",
       "\n",
       "                      stemmed_text                 lemmatized_text  \\\n",
       "0  ['how', 'are', 'you', '?', '?']         ['how', 'are', 'you??']   \n",
       "1                        ['hello']                       ['hello']   \n",
       "2        ['what', \"'s\", 'up', '?']               [\"what's\", 'up?']   \n",
       "3                        ['thank']                      ['thanks']   \n",
       "4   ['thank', 'you', 'so', 'much']  ['thank', 'you', 'so', 'much']   \n",
       "\n",
       "                     cleaned_text  \n",
       "0           ['how', 'are', 'you']  \n",
       "1                       ['hello']  \n",
       "2                 ['whats', 'up']  \n",
       "3                      ['thanks']  \n",
       "4  ['thank', 'you', 'so', 'much']  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']=data['text'].astype(str)\n",
    "data['tokenized_text']=data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9db67a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['text','intent','tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fb73d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=columns)\n",
    "output_excel=\"processed_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "92934c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(output_excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4300a",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cbbb09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SIDDESH\n",
      "[nltk_data]     VICHARE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b10c92ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['filtered_text']=data['tokenized_text'].apply(lambda tokens:[word for word in tokens if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ef9c35ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are you??</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[how, are, you, ?, ?]</td>\n",
       "      <td>[?, ?]</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['how', 'are', 'you??']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[hello]</td>\n",
       "      <td>[hello]</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[what, 's, up, ?]</td>\n",
       "      <td>['s, ?]</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"what's\", 'up?']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>['thank']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>[thank, you, so, much]</td>\n",
       "      <td>[thank, much]</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good morning</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[good, morning]</td>\n",
       "      <td>[good, morning]</td>\n",
       "      <td>['good', 'morn']</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good afternoon</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[good, afternoon]</td>\n",
       "      <td>[good, afternoon]</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how are you?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[how, are, you, ?]</td>\n",
       "      <td>[?]</td>\n",
       "      <td>['how', 'are', 'you', '?']</td>\n",
       "      <td>['how', 'are', 'you?']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[hello]</td>\n",
       "      <td>[hello]</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[what, 's, up, ?]</td>\n",
       "      <td>['s, ?]</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"what's\", 'up?']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>['thank']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>[thank, you, so, much]</td>\n",
       "      <td>[thank, much]</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good morning</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[good, morning]</td>\n",
       "      <td>[good, morning]</td>\n",
       "      <td>['good', 'morn']</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>good afternoon</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[good, afternoon]</td>\n",
       "      <td>[good, afternoon]</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bye</td>\n",
       "      <td>leaving</td>\n",
       "      <td>[bye]</td>\n",
       "      <td>[bye]</td>\n",
       "      <td>['bye']</td>\n",
       "      <td>['bye']</td>\n",
       "      <td>['bye']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>see you later</td>\n",
       "      <td>leaving</td>\n",
       "      <td>[see, you, later]</td>\n",
       "      <td>[see, later]</td>\n",
       "      <td>['see', 'you', 'later']</td>\n",
       "      <td>['see', 'you', 'later']</td>\n",
       "      <td>['see', 'you', 'later']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>talk to you later</td>\n",
       "      <td>leaving</td>\n",
       "      <td>[talk, to, you, later]</td>\n",
       "      <td>[talk, later]</td>\n",
       "      <td>['talk', 'to', 'you', 'later']</td>\n",
       "      <td>['talk', 'to', 'you', 'later']</td>\n",
       "      <td>['talk', 'to', 'you', 'later']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how's the weather</td>\n",
       "      <td>weather</td>\n",
       "      <td>[how, 's, the, weather]</td>\n",
       "      <td>['s, weather]</td>\n",
       "      <td>['how', \"'s\", 'the', 'weather']</td>\n",
       "      <td>[\"how's\", 'the', 'weather']</td>\n",
       "      <td>['hows', 'the', 'weather']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what's the weather like</td>\n",
       "      <td>weather</td>\n",
       "      <td>[what, 's, the, weather, like]</td>\n",
       "      <td>['s, weather, like]</td>\n",
       "      <td>['what', \"'s\", 'the', 'weather', 'like']</td>\n",
       "      <td>[\"what's\", 'the', 'weather', 'like']</td>\n",
       "      <td>['whats', 'the', 'weather', 'like']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>what's the temperature</td>\n",
       "      <td>weather</td>\n",
       "      <td>[what, 's, the, temperature]</td>\n",
       "      <td>['s, temperature]</td>\n",
       "      <td>['what', \"'s\", 'the', 'temperatur']</td>\n",
       "      <td>[\"what's\", 'the', 'temperature']</td>\n",
       "      <td>['whats', 'the', 'temperature']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text        intent                  tokenized_text  \\\n",
       "0             how are you??      greeting           [how, are, you, ?, ?]   \n",
       "1                     hello      greeting                         [hello]   \n",
       "2                what's up?      greeting               [what, 's, up, ?]   \n",
       "3                    thanks  appreciation                        [thanks]   \n",
       "4         thank you so much  appreciation          [thank, you, so, much]   \n",
       "5              good morning      greeting                 [good, morning]   \n",
       "6            good afternoon      greeting               [good, afternoon]   \n",
       "7              how are you?      greeting              [how, are, you, ?]   \n",
       "8                     hello      greeting                         [hello]   \n",
       "9                what's up?      greeting               [what, 's, up, ?]   \n",
       "10                   thanks  appreciation                        [thanks]   \n",
       "11        thank you so much  appreciation          [thank, you, so, much]   \n",
       "12             good morning      greeting                 [good, morning]   \n",
       "13           good afternoon      greeting               [good, afternoon]   \n",
       "14                      bye       leaving                           [bye]   \n",
       "15            see you later       leaving               [see, you, later]   \n",
       "16        talk to you later       leaving          [talk, to, you, later]   \n",
       "17        how's the weather       weather         [how, 's, the, weather]   \n",
       "18  what's the weather like       weather  [what, 's, the, weather, like]   \n",
       "19   what's the temperature       weather    [what, 's, the, temperature]   \n",
       "\n",
       "          filtered_text                              stemmed_text  \\\n",
       "0                [?, ?]           ['how', 'are', 'you', '?', '?']   \n",
       "1               [hello]                                 ['hello']   \n",
       "2               ['s, ?]                 ['what', \"'s\", 'up', '?']   \n",
       "3              [thanks]                                 ['thank']   \n",
       "4         [thank, much]            ['thank', 'you', 'so', 'much']   \n",
       "5       [good, morning]                          ['good', 'morn']   \n",
       "6     [good, afternoon]                     ['good', 'afternoon']   \n",
       "7                   [?]                ['how', 'are', 'you', '?']   \n",
       "8               [hello]                                 ['hello']   \n",
       "9               ['s, ?]                 ['what', \"'s\", 'up', '?']   \n",
       "10             [thanks]                                 ['thank']   \n",
       "11        [thank, much]            ['thank', 'you', 'so', 'much']   \n",
       "12      [good, morning]                          ['good', 'morn']   \n",
       "13    [good, afternoon]                     ['good', 'afternoon']   \n",
       "14                [bye]                                   ['bye']   \n",
       "15         [see, later]                   ['see', 'you', 'later']   \n",
       "16        [talk, later]            ['talk', 'to', 'you', 'later']   \n",
       "17        ['s, weather]           ['how', \"'s\", 'the', 'weather']   \n",
       "18  ['s, weather, like]  ['what', \"'s\", 'the', 'weather', 'like']   \n",
       "19    ['s, temperature]       ['what', \"'s\", 'the', 'temperatur']   \n",
       "\n",
       "                         lemmatized_text                         cleaned_text  \n",
       "0                ['how', 'are', 'you??']                ['how', 'are', 'you']  \n",
       "1                              ['hello']                            ['hello']  \n",
       "2                      [\"what's\", 'up?']                      ['whats', 'up']  \n",
       "3                             ['thanks']                           ['thanks']  \n",
       "4         ['thank', 'you', 'so', 'much']       ['thank', 'you', 'so', 'much']  \n",
       "5                    ['good', 'morning']                  ['good', 'morning']  \n",
       "6                  ['good', 'afternoon']                ['good', 'afternoon']  \n",
       "7                 ['how', 'are', 'you?']                ['how', 'are', 'you']  \n",
       "8                              ['hello']                            ['hello']  \n",
       "9                      [\"what's\", 'up?']                      ['whats', 'up']  \n",
       "10                            ['thanks']                           ['thanks']  \n",
       "11        ['thank', 'you', 'so', 'much']       ['thank', 'you', 'so', 'much']  \n",
       "12                   ['good', 'morning']                  ['good', 'morning']  \n",
       "13                 ['good', 'afternoon']                ['good', 'afternoon']  \n",
       "14                               ['bye']                              ['bye']  \n",
       "15               ['see', 'you', 'later']              ['see', 'you', 'later']  \n",
       "16        ['talk', 'to', 'you', 'later']       ['talk', 'to', 'you', 'later']  \n",
       "17           [\"how's\", 'the', 'weather']           ['hows', 'the', 'weather']  \n",
       "18  [\"what's\", 'the', 'weather', 'like']  ['whats', 'the', 'weather', 'like']  \n",
       "19      [\"what's\", 'the', 'temperature']      ['whats', 'the', 'temperature']  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "33fdf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save columns in the excel file\n",
    "columns=['text','intent','tokenized_text','filtered_text']\n",
    "df=pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1ba03916",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_excel=\"processed_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "50924f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(output_excel,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9e6d6897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(output_excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc206b40",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2fc0a5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [how, are, you, ?, ?]\n",
       "1                                             [hello]\n",
       "2                                   [what, 's, up, ?]\n",
       "3                                             [thank]\n",
       "4                              [thank, you, so, much]\n",
       "                            ...                      \n",
       "157    [tell, me, a, joke, to, lighten, the, mood, .]\n",
       "158                [do, you, have, a, funni, joke, ?]\n",
       "159                     [share, a, joke, with, me, .]\n",
       "160                [give, me, a, lightheart, joke, .]\n",
       "161                               [tell, me, a, joke]\n",
       "Name: stemmed_text, Length: 162, dtype: object"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['stemmed_text'] = data['tokenized_text'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "stemmed_texts = data['stemmed_text']\n",
    "stemmed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ea96574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"text\",\"intent\",\"tokenized_text\",\"filtered_text\",\"stemmed_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "13447f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "73c570bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ouput_excel='processed3_data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2a1acdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(ouput_excel,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fd967c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "65a1e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8580eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gzNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "     ---------------------------------------- 12.0/12.0 MB 7.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: spacy>=2.2.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from en-core-web-sm==2.2.0) (3.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (5.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (8.1.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (22.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.23.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (65.6.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.64.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.0->en-core-web-sm==2.2.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.0->en-core-web-sm==2.2.0) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.2.0->en-core-web-sm==2.2.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.2.0->en-core-web-sm==2.2.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.1.1)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-py3-none-any.whl size=12019114 sha256=8aeb703a5357eb0f64ac370262d9eef4eb03c0fb7222d9b3f56fa76718d636c1\n",
      "  Stored in directory: c:\\users\\siddesh vichare\\appdata\\local\\pip\\cache\\wheels\\f9\\7e\\12\\0c885b1d01a93f5cfff2e269634078c488729f52129c8f7bde\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.6.0\n",
      "    Uninstalling en-core-web-sm-3.6.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.6.0\n",
      "Successfully installed en-core-web-sm-2.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d24a2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\SIDDESH\n",
      "[nltk_data]     VICHARE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\SIDDESH\n",
      "[nltk_data]     VICHARE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6f0bed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddesh vichare\\downloads\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "15ece1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "35f32168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_path=\"processed_data.xlsx\"\n",
    "data = pd.read_excel(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d18a542e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are you??</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['?', '?']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"'s\", '?']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'much']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text        intent                   tokenized_text  \\\n",
       "0      how are you??      greeting  ['how', 'are', 'you', '?', '?']   \n",
       "1              hello      greeting                        ['hello']   \n",
       "2         what's up?      greeting        ['what', \"'s\", 'up', '?']   \n",
       "3             thanks  appreciation                       ['thanks']   \n",
       "4  thank you so much  appreciation   ['thank', 'you', 'so', 'much']   \n",
       "\n",
       "       filtered_text  \n",
       "0         ['?', '?']  \n",
       "1          ['hello']  \n",
       "2        [\"'s\", '?']  \n",
       "3         ['thanks']  \n",
       "4  ['thank', 'much']  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c65ad104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized_text'] = data['text'].apply(lambda sentence: [lemmatizer.lemmatize(token) for token in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "525e0080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 [how, are, you??]\n",
       "1                                           [hello]\n",
       "2                                     [what's, up?]\n",
       "3                                          [thanks]\n",
       "4                            [thank, you, so, much]\n",
       "                           ...                     \n",
       "157    [tell, me, a, joke, to, lighten, the, mood.]\n",
       "158                [do, you, have, a, funny, joke?]\n",
       "159                     [share, a, joke, with, me.]\n",
       "160              [give, me, a, lighthearted, joke.]\n",
       "161                             [tell, me, a, joke]\n",
       "Name: lemmatized_text, Length: 162, dtype: object"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4f04b5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are you??</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['?', '?']</td>\n",
       "      <td>[how, are, you??]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>[hello]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"'s\", '?']</td>\n",
       "      <td>[what's, up?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>[thanks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'much']</td>\n",
       "      <td>[thank, you, so, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good morning</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "      <td>[good, morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good afternoon</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>[good, afternoon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how are you?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['how', 'are', 'you', '?']</td>\n",
       "      <td>['?']</td>\n",
       "      <td>[how, are, you?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>[hello]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"'s\", '?']</td>\n",
       "      <td>[what's, up?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>[thanks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'much']</td>\n",
       "      <td>[thank, you, so, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good morning</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "      <td>['good', 'morning']</td>\n",
       "      <td>[good, morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>good afternoon</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>['good', 'afternoon']</td>\n",
       "      <td>[good, afternoon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bye</td>\n",
       "      <td>leaving</td>\n",
       "      <td>['bye']</td>\n",
       "      <td>['bye']</td>\n",
       "      <td>[bye]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>see you later</td>\n",
       "      <td>leaving</td>\n",
       "      <td>['see', 'you', 'later']</td>\n",
       "      <td>['see', 'later']</td>\n",
       "      <td>[see, you, later]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>talk to you later</td>\n",
       "      <td>leaving</td>\n",
       "      <td>['talk', 'to', 'you', 'later']</td>\n",
       "      <td>['talk', 'later']</td>\n",
       "      <td>[talk, to, you, later]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how's the weather</td>\n",
       "      <td>weather</td>\n",
       "      <td>['how', \"'s\", 'the', 'weather']</td>\n",
       "      <td>[\"'s\", 'weather']</td>\n",
       "      <td>[how's, the, weather]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what's the weather like</td>\n",
       "      <td>weather</td>\n",
       "      <td>['what', \"'s\", 'the', 'weather', 'like']</td>\n",
       "      <td>[\"'s\", 'weather', 'like']</td>\n",
       "      <td>[what's, the, weather, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>what's the temperature</td>\n",
       "      <td>weather</td>\n",
       "      <td>['what', \"'s\", 'the', 'temperature']</td>\n",
       "      <td>[\"'s\", 'temperature']</td>\n",
       "      <td>[what's, the, temperature]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text        intent  \\\n",
       "0             how are you??      greeting   \n",
       "1                     hello      greeting   \n",
       "2                what's up?      greeting   \n",
       "3                    thanks  appreciation   \n",
       "4         thank you so much  appreciation   \n",
       "5              good morning      greeting   \n",
       "6            good afternoon      greeting   \n",
       "7              how are you?      greeting   \n",
       "8                     hello      greeting   \n",
       "9                what's up?      greeting   \n",
       "10                   thanks  appreciation   \n",
       "11        thank you so much  appreciation   \n",
       "12             good morning      greeting   \n",
       "13           good afternoon      greeting   \n",
       "14                      bye       leaving   \n",
       "15            see you later       leaving   \n",
       "16        talk to you later       leaving   \n",
       "17        how's the weather       weather   \n",
       "18  what's the weather like       weather   \n",
       "19   what's the temperature       weather   \n",
       "\n",
       "                              tokenized_text              filtered_text  \\\n",
       "0            ['how', 'are', 'you', '?', '?']                 ['?', '?']   \n",
       "1                                  ['hello']                  ['hello']   \n",
       "2                  ['what', \"'s\", 'up', '?']                [\"'s\", '?']   \n",
       "3                                 ['thanks']                 ['thanks']   \n",
       "4             ['thank', 'you', 'so', 'much']          ['thank', 'much']   \n",
       "5                        ['good', 'morning']        ['good', 'morning']   \n",
       "6                      ['good', 'afternoon']      ['good', 'afternoon']   \n",
       "7                 ['how', 'are', 'you', '?']                      ['?']   \n",
       "8                                  ['hello']                  ['hello']   \n",
       "9                  ['what', \"'s\", 'up', '?']                [\"'s\", '?']   \n",
       "10                                ['thanks']                 ['thanks']   \n",
       "11            ['thank', 'you', 'so', 'much']          ['thank', 'much']   \n",
       "12                       ['good', 'morning']        ['good', 'morning']   \n",
       "13                     ['good', 'afternoon']      ['good', 'afternoon']   \n",
       "14                                   ['bye']                    ['bye']   \n",
       "15                   ['see', 'you', 'later']           ['see', 'later']   \n",
       "16            ['talk', 'to', 'you', 'later']          ['talk', 'later']   \n",
       "17           ['how', \"'s\", 'the', 'weather']          [\"'s\", 'weather']   \n",
       "18  ['what', \"'s\", 'the', 'weather', 'like']  [\"'s\", 'weather', 'like']   \n",
       "19      ['what', \"'s\", 'the', 'temperature']      [\"'s\", 'temperature']   \n",
       "\n",
       "                 lemmatized_text  \n",
       "0              [how, are, you??]  \n",
       "1                        [hello]  \n",
       "2                  [what's, up?]  \n",
       "3                       [thanks]  \n",
       "4         [thank, you, so, much]  \n",
       "5                [good, morning]  \n",
       "6              [good, afternoon]  \n",
       "7               [how, are, you?]  \n",
       "8                        [hello]  \n",
       "9                  [what's, up?]  \n",
       "10                      [thanks]  \n",
       "11        [thank, you, so, much]  \n",
       "12               [good, morning]  \n",
       "13             [good, afternoon]  \n",
       "14                         [bye]  \n",
       "15             [see, you, later]  \n",
       "16        [talk, to, you, later]  \n",
       "17         [how's, the, weather]  \n",
       "18  [what's, the, weather, like]  \n",
       "19    [what's, the, temperature]  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e4a5e47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'running']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lem = WordNetLemmatizer()\n",
    "lemmatized_words = [lem.lemmatize(word) for word in \"I'm running\".split()]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "41929927",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['text','intent','tokenized_text','filtered_text','stemmed_text','lemmatized_text']\n",
    "df=pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5c0a536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ouput_excel='processed4_data.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff0090",
   "metadata": {},
   "source": [
    "# Removing Punctuation and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3fa6113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a81b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words_list):\n",
    "    #Define pattern whcih is all Alphanumerical character\n",
    "    # With the \"r\" prefix (raw string)\n",
    "    pattern=r'[^a-zA-Z0-9\\s]'\n",
    "    clean_words=[re.sub(pattern,\"\",word) for word in words_list]\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ef3739c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text']=data['lemmatized_text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "aa4e2b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [how, are, you]\n",
       "1                                          [hello]\n",
       "2                                      [whats, up]\n",
       "3                                         [thanks]\n",
       "4                           [thank, you, so, much]\n",
       "                          ...                     \n",
       "157    [tell, me, a, joke, to, lighten, the, mood]\n",
       "158                [do, you, have, a, funny, joke]\n",
       "159                     [share, a, joke, with, me]\n",
       "160              [give, me, a, lighthearted, joke]\n",
       "161                            [tell, me, a, joke]\n",
       "Name: cleaned_text, Length: 162, dtype: object"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f6ee4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['text','intent','tokenized_text','filtered_text','stemmed_text','lemmatized_text','cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "81339b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c153c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_excel=\"processed6_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9be0da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['joined_text']=data['cleaned_text'].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ace0f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"text\",\"intent\",'tokenized_text','stemmed_text','lemmatized_text','cleaned_text','joined_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ef62ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=columns)\n",
    "output_excel='processed9_data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "394b9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(output_excel,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c16dd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path='processed9_data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "52370dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "af7f78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9553b9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>joined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are you??</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['how', 'are', 'you??']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"what's\", 'up?']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thank']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text        intent                   tokenized_text  \\\n",
       "0      how are you??      greeting  ['how', 'are', 'you', '?', '?']   \n",
       "1              hello      greeting                        ['hello']   \n",
       "2         what's up?      greeting        ['what', \"'s\", 'up', '?']   \n",
       "3             thanks  appreciation                       ['thanks']   \n",
       "4  thank you so much  appreciation   ['thank', 'you', 'so', 'much']   \n",
       "\n",
       "                      stemmed_text                 lemmatized_text  \\\n",
       "0  ['how', 'are', 'you', '?', '?']         ['how', 'are', 'you??']   \n",
       "1                        ['hello']                       ['hello']   \n",
       "2        ['what', \"'s\", 'up', '?']               [\"what's\", 'up?']   \n",
       "3                        ['thank']                      ['thanks']   \n",
       "4   ['thank', 'you', 'so', 'much']  ['thank', 'you', 'so', 'much']   \n",
       "\n",
       "                     cleaned_text                     joined_text  \n",
       "0           ['how', 'are', 'you']           ['how', 'are', 'you']  \n",
       "1                       ['hello']                       ['hello']  \n",
       "2                 ['whats', 'up']                 ['whats', 'up']  \n",
       "3                      ['thanks']                      ['thanks']  \n",
       "4  ['thank', 'you', 'so', 'much']  ['thank', 'you', 'so', 'much']  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "16faaa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the joined_text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['joined_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF DataFrame with the original data\n",
    "data_with_tfidf = pd.concat([data, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1bbedd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>joined_text</th>\n",
       "      <th>about</th>\n",
       "      <th>active</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>where</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>words</th>\n",
       "      <th>year</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how are you??</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['how', 'are', 'you', '?', '?']</td>\n",
       "      <td>['how', 'are', 'you??']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "      <td>['how', 'are', 'you']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>['hello']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's up?</td>\n",
       "      <td>greeting</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>['what', \"'s\", 'up', '?']</td>\n",
       "      <td>[\"what's\", 'up?']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "      <td>['whats', 'up']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thank']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>['thanks']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you so much</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>['thank', 'you', 'so', 'much']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.618708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows  162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text        intent                   tokenized_text  \\\n",
       "0        how are you??      greeting  ['how', 'are', 'you', '?', '?']   \n",
       "1                hello      greeting                        ['hello']   \n",
       "2           what's up?      greeting        ['what', \"'s\", 'up', '?']   \n",
       "3               thanks  appreciation                       ['thanks']   \n",
       "4    thank you so much  appreciation   ['thank', 'you', 'so', 'much']   \n",
       "..                 ...           ...                              ...   \n",
       "157                NaN           NaN                              NaN   \n",
       "158                NaN           NaN                              NaN   \n",
       "159                NaN           NaN                              NaN   \n",
       "160                NaN           NaN                              NaN   \n",
       "161                NaN           NaN                              NaN   \n",
       "\n",
       "                        stemmed_text                 lemmatized_text  \\\n",
       "0    ['how', 'are', 'you', '?', '?']         ['how', 'are', 'you??']   \n",
       "1                          ['hello']                       ['hello']   \n",
       "2          ['what', \"'s\", 'up', '?']               [\"what's\", 'up?']   \n",
       "3                          ['thank']                      ['thanks']   \n",
       "4     ['thank', 'you', 'so', 'much']  ['thank', 'you', 'so', 'much']   \n",
       "..                               ...                             ...   \n",
       "157                              NaN                             NaN   \n",
       "158                              NaN                             NaN   \n",
       "159                              NaN                             NaN   \n",
       "160                              NaN                             NaN   \n",
       "161                              NaN                             NaN   \n",
       "\n",
       "                       cleaned_text                     joined_text  about  \\\n",
       "0             ['how', 'are', 'you']           ['how', 'are', 'you']    NaN   \n",
       "1                         ['hello']                       ['hello']    NaN   \n",
       "2                   ['whats', 'up']                 ['whats', 'up']    NaN   \n",
       "3                        ['thanks']                      ['thanks']    NaN   \n",
       "4    ['thank', 'you', 'so', 'much']  ['thank', 'you', 'so', 'much']    NaN   \n",
       "..                              ...                             ...    ...   \n",
       "157                             NaN                             NaN    0.0   \n",
       "158                             NaN                             NaN    0.0   \n",
       "159                             NaN                             NaN    0.0   \n",
       "160                             NaN                             NaN    0.0   \n",
       "161                             NaN                             NaN    0.0   \n",
       "\n",
       "     active  add  ...  where  who  will      with  words  year       you  \\\n",
       "0       NaN  NaN  ...    NaN  NaN   NaN       NaN    NaN   NaN       NaN   \n",
       "1       NaN  NaN  ...    NaN  NaN   NaN       NaN    NaN   NaN       NaN   \n",
       "2       NaN  NaN  ...    NaN  NaN   NaN       NaN    NaN   NaN       NaN   \n",
       "3       NaN  NaN  ...    NaN  NaN   NaN       NaN    NaN   NaN       NaN   \n",
       "4       NaN  NaN  ...    NaN  NaN   NaN       NaN    NaN   NaN       NaN   \n",
       "..      ...  ...  ...    ...  ...   ...       ...    ...   ...       ...   \n",
       "157     0.0  0.0  ...    0.0  0.0   0.0  0.000000    0.0   0.0  0.000000   \n",
       "158     0.0  0.0  ...    0.0  0.0   0.0  0.000000    0.0   0.0  0.239261   \n",
       "159     0.0  0.0  ...    0.0  0.0   0.0  0.618708    0.0   0.0  0.000000   \n",
       "160     0.0  0.0  ...    0.0  0.0   0.0  0.000000    0.0   0.0  0.000000   \n",
       "161     0.0  0.0  ...    0.0  0.0   0.0  0.000000    0.0   0.0  0.000000   \n",
       "\n",
       "     your  youre  youtube  \n",
       "0     NaN    NaN      NaN  \n",
       "1     NaN    NaN      NaN  \n",
       "2     NaN    NaN      NaN  \n",
       "3     NaN    NaN      NaN  \n",
       "4     NaN    NaN      NaN  \n",
       "..    ...    ...      ...  \n",
       "157   0.0    0.0      0.0  \n",
       "158   0.0    0.0      0.0  \n",
       "159   0.0    0.0      0.0  \n",
       "160   0.0    0.0      0.0  \n",
       "161   0.0    0.0      0.0  \n",
       "\n",
       "[324 rows x 162 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "14bc380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fir and Transform the joined_text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['joined_text'])\n",
    "\n",
    "#Convert the TF-IDF matrix to DataFrame\n",
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(),columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "data_with_tfidf=pd.concat([data,tfidf_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d877d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
